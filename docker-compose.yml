services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_HEAP_OPTS: "-Xmx512M -Xms256M"
      # Ważne! Wymaga nowego woluminu bez istniejącego ID klastra
      KAFKA_OPTS: "-Dlog4j.rootLogger=INFO"
    restart: always
    user: root
    volumes:
      - kafka_data_clean:/var/lib/kafka/data
      - ./kafka-init.sh:/kafka-init.sh
    command: ["/bin/bash", "/kafka-init.sh"]

  postgres_db:
    image: postgres:15
    container_name: postgres_db
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: postgres
      POSTGRES_INITDB_ARGS: "--data-checksums"
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./postgresql.conf:/etc/postgresql/postgresql.conf
    command: ["postgres", "-c", "config_file=/etc/postgresql/postgresql.conf"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 5

  debezium:
    image: debezium/connect:2.3
    container_name: debezium
    depends_on:
      - postgres_db
      - kafka
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: my_connect_configs
      OFFSET_STORAGE_TOPIC: my_connect_offsets
      STATUS_STORAGE_TOPIC: my_connect_statuses
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_REST_ADVERTISED_HOST_NAME: localhost
      CONNECT_HOST_NAME: debezium
      KAFKA_OPTS: "-Dsun.net.inetaddr.ttl=30 -Dsun.net.inetaddr.negative.ttl=30"
    links:
      - kafka
    restart: always

  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "8080:8080"
      - "7077:7077"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077

  csv_import_app:
    build: .
    container_name: csv_import_app
    depends_on:
      - postgres_db
    environment:
      DB_USER: postgres
      DB_PASSWORD: password
      DB_HOST: postgres_db
      DB_PORT: 5432
      DB_NAME: postgres
      PYTHONDONTWRITEBYTECODE: 1
    volumes:
      - ./csv_files:/app/csv_files
    command: ["python", "import_csv_to_postgres.py", "--folder", "csv_files", "--db_host", "postgres_db"]

  spark-app:
    build:
      context: ./spark-app
    container_name: spark-app
    depends_on:
      - spark-master
      - kafka
      - minio
      - debezium
    volumes:
      - ./spark-app:/app
    links:
      - debezium
      - kafka
      - postgres_db
    environment:
      DEBEZIUM_HOST: debezium
      KAFKA_HOST: kafka
      MINIO_HOST: minio
      POSTGRES_HOST: postgres_db
    restart: always

volumes:
  pgdata:
  minio_data:
  kafka_data_clean:  # Nowa nazwa woluminu, aby uniknąć konfliktu ze starymi danymi
    name: kafka_data_clean

networks:
  default:
    driver: bridge